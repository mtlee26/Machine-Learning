{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:50:23.748199Z",
     "iopub.status.busy": "2024-12-19T17:50:23.747629Z",
     "iopub.status.idle": "2024-12-19T17:50:32.273798Z",
     "shell.execute_reply": "2024-12-19T17:50:32.272899Z",
     "shell.execute_reply.started": "2024-12-19T17:50:23.748161Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.base import clone\n",
    "import optuna\n",
    "from scipy.optimize import minimize\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import make_classification\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "n_splits = 5\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:50:41.886214Z",
     "iopub.status.busy": "2024-12-19T17:50:41.885360Z",
     "iopub.status.idle": "2024-12-19T17:50:41.912313Z",
     "shell.execute_reply": "2024-12-19T17:50:41.911254Z",
     "shell.execute_reply.started": "2024-12-19T17:50:41.886184Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed_everything(2024)\n",
    "\n",
    "#Basic_demos_sex, cgas-cgas_score, max_stage, endurance-time_mins, fgc-fgc_srl_zone\n",
    "\n",
    "train_featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'BMI_PHR']\n",
    "\n",
    "test_featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n",
    "                'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n",
    "                'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW', 'BMI_PHR']\n",
    "\n",
    "\n",
    "Params = {\n",
    "    'learning_rate': 0.046,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 478,\n",
    "    'min_data_in_leaf': 13,\n",
    "    'feature_fraction': 0.893,\n",
    "    'bagging_fraction': 0.784,\n",
    "    'bagging_freq': 4,\n",
    "    'lambda_l1': 10,  # Increased from 6.59\n",
    "    'lambda_l2': 0.01,  # Increased from 2.68e-06\n",
    "  #  'device': 'cpu',\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "XGB_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'max_depth': 6,\n",
    "    'n_estimators': 200,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 1,  # Increased from 0.1\n",
    "    'reg_lambda': 5,  # Increased from 1\n",
    "    'random_state': SEED,\n",
    "   # 'tree_method': 'gpu_hist'\n",
    "}\n",
    "\n",
    "\n",
    "CatBoost_Params = {\n",
    "    'learning_rate': 0.05,\n",
    "    'depth': 6,\n",
    "    'iterations': 200,\n",
    "    'random_seed': SEED,\n",
    "    'verbose': 0,\n",
    "    'l2_leaf_reg': 10,  # Increase this value\n",
    "    #'task_type': 'GPU',\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_feature(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    # cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', \n",
    "    #       'FGC-Season', 'BIA-Season', 'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "    # for c in cat_c: \n",
    "    #     df[c] = df[c].fillna('Missing')\n",
    "    #     df[c] = df[c].astype('category')\n",
    "        \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n",
    "    df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n",
    "    df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n",
    "    df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n",
    "    df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n",
    "    df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n",
    "    df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n",
    "    df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n",
    "    df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n",
    "    df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n",
    "    df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n",
    "    df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n",
    "    df['BMI_PHR'] = df['Physical-BMI'] * df['Physical-HeartRate']\n",
    "    \n",
    "    return df\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2+10),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(encoding_dim*2+10, encoding_dim),\n",
    "            nn.LeakyReLU()\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, encoding_dim+15),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(encoding_dim+15, encoding_dim*3),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(encoding_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# encoder du lieu\n",
    "def autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:50:41.914299Z",
     "iopub.status.busy": "2024-12-19T17:50:41.913940Z",
     "iopub.status.idle": "2024-12-19T17:50:41.927919Z",
     "shell.execute_reply": "2024-12-19T17:50:41.926832Z",
     "shell.execute_reply.started": "2024-12-19T17:50:41.914262Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# thong ke parquet file\n",
    "def pre_progress_parquet_data(file_name, parquet_dir):\n",
    "    file_path = os.path.join(parquet_dir, file_name, 'part-0.parquet')\n",
    "    df = pd.read_parquet(file_path)\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), file_name.split('=')[1]\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: pre_progress_parquet_data(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    stats, indexes = zip(*results)\n",
    "    \n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:50:41.929433Z",
     "iopub.status.busy": "2024-12-19T17:50:41.929027Z",
     "iopub.status.idle": "2024-12-19T17:52:05.686436Z",
     "shell.execute_reply": "2024-12-19T17:52:05.685395Z",
     "shell.execute_reply.started": "2024-12-19T17:50:41.929395Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# data loader\n",
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "#test = train.head(20)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Giả sử `df` là bảng dữ liệu\n",
    "train_non_id = train.drop(\"id\", axis=1)\n",
    "train_non_id = preprocess_feature(train_non_id)\n",
    "\n",
    "train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n",
    "#test_ts = train_ts.head(20)\n",
    "df_train = train_ts.drop('id', axis=1) \n",
    "df_test = test_ts.drop('id', axis=1) # bo cot id\n",
    "\n",
    "scaler = StandardScaler() # scale du lieu\n",
    "df_scaled = scaler.fit_transform(df_train)\n",
    "data_tensor = torch.FloatTensor(df_scaled)\n",
    "df_scaled2 = scaler.fit_transform(df_test)  \n",
    "data_tensor2 = torch.FloatTensor(df_scaled2)\n",
    "\n",
    "input_dim = data_tensor.shape[1]\n",
    "autoencoder = AutoEncoder(input_dim, 60)\n",
    "\n",
    "# encoding ...\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "for epoch in range(100):\n",
    "    for i in range(0, len(data_tensor), 32):\n",
    "        batch = data_tensor[i : i + 32]\n",
    "        optimizer.zero_grad()\n",
    "        reconstructed = autoencoder(batch)\n",
    "        loss = criterion(reconstructed, batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{100}], Loss: {loss.item():.4f}]')\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "    encoded_data2= autoencoder.encoder(data_tensor2).numpy()\n",
    "\n",
    "# chuyen ve dang dataframe\n",
    "train_ts_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "test_ts_encoded = pd.DataFrame(encoded_data2, columns=[f'Enc_{i + 1}' for i in range(encoded_data2.shape[1])])\n",
    "\n",
    "time_series_cols = train_ts_encoded.columns.tolist()\n",
    "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "test = pd.merge(test, train_ts_encoded, how=\"left\", on='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:05.688153Z",
     "iopub.status.busy": "2024-12-19T17:52:05.687467Z",
     "iopub.status.idle": "2024-12-19T17:52:05.820683Z",
     "shell.execute_reply": "2024-12-19T17:52:05.819743Z",
     "shell.execute_reply.started": "2024-12-19T17:52:05.688124Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:05.822032Z",
     "iopub.status.busy": "2024-12-19T17:52:05.821752Z",
     "iopub.status.idle": "2024-12-19T17:52:13.333384Z",
     "shell.execute_reply": "2024-12-19T17:52:13.332346Z",
     "shell.execute_reply.started": "2024-12-19T17:52:05.822007Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = train.select_dtypes(include=['int32', 'int64', 'float64', 'int64']).columns\n",
    "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
    "for col in train.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train[col]\n",
    "        \n",
    "train = train_imputed\n",
    "\n",
    "train = preprocess_feature(train)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = preprocess_feature(test)\n",
    "\n",
    "# def create_mapping(column, dataset):\n",
    "#     unique_values = dataset[column].unique()\n",
    "#     return {value: idx for idx, value in enumerate(unique_values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:13.334728Z",
     "iopub.status.busy": "2024-12-19T17:52:13.334480Z",
     "iopub.status.idle": "2024-12-19T17:52:13.499182Z",
     "shell.execute_reply": "2024-12-19T17:52:13.498227Z",
     "shell.execute_reply.started": "2024-12-19T17:52:13.334707Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:13.500708Z",
     "iopub.status.busy": "2024-12-19T17:52:13.500337Z",
     "iopub.status.idle": "2024-12-19T17:52:13.628699Z",
     "shell.execute_reply": "2024-12-19T17:52:13.627756Z",
     "shell.execute_reply.started": "2024-12-19T17:52:13.500672Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:13.631811Z",
     "iopub.status.busy": "2024-12-19T17:52:13.631519Z",
     "iopub.status.idle": "2024-12-19T17:52:13.647494Z",
     "shell.execute_reply": "2024-12-19T17:52:13.646406Z",
     "shell.execute_reply.started": "2024-12-19T17:52:13.631783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_featuresCols += time_series_cols\n",
    "\n",
    "train = train[train_featuresCols]\n",
    "# drop cac ban ghi co sii nan\n",
    "train = train.dropna(subset='sii') \n",
    "\n",
    "test_featuresCols += time_series_cols\n",
    "test = test[test_featuresCols]\n",
    "\n",
    "# con cai nao to qua thi nan\n",
    "if np.any(np.isinf(train)):\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "if np.any(np.isinf(test)):\n",
    "    test = test.replace([np.inf, -np.inf], np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:13.649516Z",
     "iopub.status.busy": "2024-12-19T17:52:13.649133Z",
     "iopub.status.idle": "2024-12-19T17:52:13.662004Z",
     "shell.execute_reply": "2024-12-19T17:52:13.660960Z",
     "shell.execute_reply.started": "2024-12-19T17:52:13.649477Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def eval_models(thresholds, y, y_predict_non_rounded):\n",
    "    y_predic = np.where(y_predict_non_rounded < thresholds[0], 0,\n",
    "                    np.where(y_predict_non_rounded < thresholds[1], 1,\n",
    "                             np.where(y_predict_non_rounded < thresholds[2], 2, 3)))\n",
    "    return -cohen_kappa_score(y, y_predic, weights='quadratic')\n",
    "\n",
    "def train_models(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        y_train_pred_rounded = y_train_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = cohen_kappa_score(y_train, y_train_pred_rounded, weights='quadratic')        \n",
    "        val_kappa = cohen_kappa_score(y_val, y_val_pred_rounded, weights='quadratic')\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(eval_models,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = np.where(oof_non_rounded < (KappaOPtimizer.x)[0], 0,\n",
    "                    np.where(oof_non_rounded < (KappaOPtimizer.x)[1], 1,\n",
    "                             np.where(oof_non_rounded < (KappaOPtimizer.x)[2], 2, 3)))\n",
    "    \n",
    "    tKappa = cohen_kappa_score(y, oof_tuned, weights='quadratic')\n",
    "\n",
    "    print(f\"----> || Optimized SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tpTuned = np.where(tpm < (KappaOPtimizer.x)[0], 0,\n",
    "                    np.where(tpm < (KappaOPtimizer.x)[1], 1,\n",
    "                             np.where(tpm < (KappaOPtimizer.x)[2], 2, 3)))\n",
    "    \n",
    "    submission = pd.DataFrame({\n",
    "        'id': sample['id'],\n",
    "        'sii': tpTuned\n",
    "    })\n",
    "\n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:13.663315Z",
     "iopub.status.busy": "2024-12-19T17:52:13.662947Z",
     "iopub.status.idle": "2024-12-19T17:52:13.684654Z",
     "shell.execute_reply": "2024-12-19T17:52:13.683567Z",
     "shell.execute_reply.started": "2024-12-19T17:52:13.663285Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# loading model\n",
    "Light = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\n",
    "XGB_Model = XGBRegressor(**XGB_Params)\n",
    "CatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n",
    "\n",
    "voting_model = VotingRegressor(estimators=[\n",
    "    ('lightgbm', Light),\n",
    "    ('xgboost', XGB_Model),\n",
    "    ('catboost', CatBoost_Model),\n",
    "],weights=[4.0,5.0,6.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:13.686152Z",
     "iopub.status.busy": "2024-12-19T17:52:13.685736Z",
     "iopub.status.idle": "2024-12-19T17:52:13.794274Z",
     "shell.execute_reply": "2024-12-19T17:52:13.793280Z",
     "shell.execute_reply.started": "2024-12-19T17:52:13.686088Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:13.795515Z",
     "iopub.status.busy": "2024-12-19T17:52:13.795194Z",
     "iopub.status.idle": "2024-12-19T17:52:13.921895Z",
     "shell.execute_reply": "2024-12-19T17:52:13.920938Z",
     "shell.execute_reply.started": "2024-12-19T17:52:13.795487Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:52:13.923174Z",
     "iopub.status.busy": "2024-12-19T17:52:13.922850Z",
     "iopub.status.idle": "2024-12-19T17:53:16.926443Z",
     "shell.execute_reply": "2024-12-19T17:53:16.925373Z",
     "shell.execute_reply.started": "2024-12-19T17:52:13.923148Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# train\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n",
    "Submission1 = train_models(voting_model, test)\n",
    "\n",
    "# Save submission\n",
    "Submission1.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:53:16.927513Z",
     "iopub.status.busy": "2024-12-19T17:53:16.927256Z",
     "iopub.status.idle": "2024-12-19T17:53:16.936042Z",
     "shell.execute_reply": "2024-12-19T17:53:16.934950Z",
     "shell.execute_reply.started": "2024-12-19T17:53:16.927481Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Submission1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:53:16.937302Z",
     "iopub.status.busy": "2024-12-19T17:53:16.936974Z",
     "iopub.status.idle": "2024-12-19T17:53:17.014043Z",
     "shell.execute_reply": "2024-12-19T17:53:17.013273Z",
     "shell.execute_reply.started": "2024-12-19T17:53:16.937266Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n",
    "sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:53:17.015230Z",
     "iopub.status.busy": "2024-12-19T17:53:17.014909Z",
     "iopub.status.idle": "2024-12-19T17:55:30.729600Z",
     "shell.execute_reply": "2024-12-19T17:55:30.728715Z",
     "shell.execute_reply.started": "2024-12-19T17:53:17.015197Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n",
    "                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n",
    "                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n",
    "\n",
    "cat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n",
    "          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n",
    "          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n",
    "\n",
    "time_series_cols = train_ts.columns.tolist()\n",
    "time_series_cols.remove(\"id\")\n",
    "\n",
    "train = pd.merge(train, train_ts, how=\"left\", on='id')\n",
    "test = pd.merge(test, test_ts, how=\"left\", on='id')\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test = test.drop('id', axis=1)\n",
    "\n",
    "featuresCols += time_series_cols\n",
    "\n",
    "train = train[featuresCols]\n",
    "train = train.dropna(subset='sii')\n",
    "\n",
    "def update(df):\n",
    "    for c in cat_c: \n",
    "        df[c] = df[c].fillna('Missing')\n",
    "        df[c] = df[c].astype('category')\n",
    "    return df\n",
    "\n",
    "train = update(train)\n",
    "test = update(test)\n",
    "\n",
    "def create_mapping(column, dataset):\n",
    "    unique_values = dataset[column].unique()\n",
    "    return {value: idx for idx, value in enumerate(unique_values)}\n",
    "\n",
    "for col in cat_c:\n",
    "    mapping = create_mapping(col, train)\n",
    "    mappingTe = create_mapping(col, test)\n",
    "    \n",
    "    train[col] = train[col].replace(mapping).astype(int)\n",
    "    test[col] = test[col].replace(mappingTe).astype(int)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def Train_model(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "    return tp_rounded\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n",
    "    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n",
    "    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n",
    "    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n",
    "    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n",
    "])\n",
    "\n",
    "Submission2 = Train_model(ensemble, test)\n",
    "Submission2 = pd.DataFrame({\n",
    "    'id': sample['id'],\n",
    "    'sii': Submission2\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:55:30.730787Z",
     "iopub.status.busy": "2024-12-19T17:55:30.730544Z",
     "iopub.status.idle": "2024-12-19T17:55:30.739195Z",
     "shell.execute_reply": "2024-12-19T17:55:30.738350Z",
     "shell.execute_reply.started": "2024-12-19T17:55:30.730767Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "Submission2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:55:30.740263Z",
     "iopub.status.busy": "2024-12-19T17:55:30.739987Z",
     "iopub.status.idle": "2024-12-19T17:55:30.766217Z",
     "shell.execute_reply": "2024-12-19T17:55:30.765247Z",
     "shell.execute_reply.started": "2024-12-19T17:55:30.740232Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "sub1 = Submission1\n",
    "#sub2 = Submission2\n",
    "sub2 = Submission2\n",
    "\n",
    "sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n",
    "#sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n",
    "\n",
    "combined = pd.DataFrame({\n",
    "    'id': sub1['id'],\n",
    "    'sii_1': sub1['sii'],\n",
    "    #'sii_2': sub2['sii'],\n",
    "    'sii_2': sub2['sii']\n",
    "})\n",
    "\n",
    "def majority_vote(row):\n",
    "    return row.mode()[0]\n",
    "\n",
    "combined['final_sii'] = combined[['sii_1', 'sii_2']].apply(majority_vote, axis=1)\n",
    "\n",
    "final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n",
    "\n",
    "final_submission.to_csv('submission.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:55:30.767475Z",
     "iopub.status.busy": "2024-12-19T17:55:30.767116Z",
     "iopub.status.idle": "2024-12-19T17:55:30.781440Z",
     "shell.execute_reply": "2024-12-19T17:55:30.780412Z",
     "shell.execute_reply.started": "2024-12-19T17:55:30.767439Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-19T17:55:30.782635Z",
     "iopub.status.busy": "2024-12-19T17:55:30.782381Z",
     "iopub.status.idle": "2024-12-19T17:55:30.798240Z",
     "shell.execute_reply": "2024-12-19T17:55:30.797355Z",
     "shell.execute_reply.started": "2024-12-19T17:55:30.782612Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "final_submission['sii'].value_counts()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 9643020,
     "sourceId": 81933,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30822,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
